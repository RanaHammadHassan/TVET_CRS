{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94f63e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7616e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_excel('Tvet_dataRegular.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2557b334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProfileCode</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Score_E</th>\n",
       "      <th>Score_A</th>\n",
       "      <th>Score_C</th>\n",
       "      <th>Score_N</th>\n",
       "      <th>Score_O</th>\n",
       "      <th>Result</th>\n",
       "      <th>Trade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>711</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>85</td>\n",
       "      <td>Auto Mechanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>712</td>\n",
       "      <td>Male</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>79</td>\n",
       "      <td>Auto Mechanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>717</td>\n",
       "      <td>Male</td>\n",
       "      <td>22</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>80</td>\n",
       "      <td>Auto Mechanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>720</td>\n",
       "      <td>Male</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>95</td>\n",
       "      <td>Auto Mechanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>723</td>\n",
       "      <td>Male</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>80</td>\n",
       "      <td>Auto Mechanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>5615</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>95</td>\n",
       "      <td>Refrigeration &amp; Air Conditioning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>5616</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>96</td>\n",
       "      <td>Refrigeration &amp; Air Conditioning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>5618</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>94</td>\n",
       "      <td>Refrigeration &amp; Air Conditioning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>5620</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>93</td>\n",
       "      <td>Refrigeration &amp; Air Conditioning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>5623</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>90</td>\n",
       "      <td>Refrigeration &amp; Air Conditioning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>551 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ProfileCode Gender  Age  Score_E  Score_A  Score_C  Score_N  Score_O  \\\n",
       "0            711   Male   21       31       32       38       31       35   \n",
       "1            712   Male   16       13       16       21       11       27   \n",
       "2            717   Male   22       36       36       38       21       32   \n",
       "3            720   Male   17       19       29       26       23       29   \n",
       "4            723   Male   17       16       35       30       20       24   \n",
       "..           ...    ...  ...      ...      ...      ...      ...      ...   \n",
       "546         5615   Male   21       21       34       36       22       21   \n",
       "547         5616   Male   19       22       22       36       28       25   \n",
       "548         5618   Male   20       22       34       26        8       28   \n",
       "549         5620   Male   19       11       23       31       31       26   \n",
       "550         5623   Male   18       25       28       33        7       26   \n",
       "\n",
       "     Result                             Trade  \n",
       "0        85                     Auto Mechanic  \n",
       "1        79                     Auto Mechanic  \n",
       "2        80                     Auto Mechanic  \n",
       "3        95                     Auto Mechanic  \n",
       "4        80                     Auto Mechanic  \n",
       "..      ...                               ...  \n",
       "546      95  Refrigeration & Air Conditioning  \n",
       "547      96  Refrigeration & Air Conditioning  \n",
       "548      94  Refrigeration & Air Conditioning  \n",
       "549      93  Refrigeration & Air Conditioning  \n",
       "550      90  Refrigeration & Air Conditioning  \n",
       "\n",
       "[551 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "895075c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "le_gender = LabelEncoder()\n",
    "df['Gender'] = le_gender.fit_transform(df['Gender'])\n",
    "\n",
    "le_trade = LabelEncoder()\n",
    "df['Trade'] = le_trade.fit_transform(df['Trade'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('Trade', axis=1)\n",
    "y = df['Trade']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert target to categorical\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "563c8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the hyperparameter tuning model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=hp.Int('units_input', min_value=32, max_value=256, step=32), input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_input', 0.1, 0.5, step=0.1)))\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)))\n",
    "    model.add(Dense(y_train_categorical.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe3e6da7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fine-tune the best model if necessary\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_categorical, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Fine-tune the best model if necessary\n",
    "history = best_model.fit(X_train, y_train_categorical, epochs=100, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc64ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.1184 - loss: 3.4404 - val_accuracy: 0.2500 - val_loss: 2.3489 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2122 - loss: 2.9989 - val_accuracy: 0.2727 - val_loss: 2.2932 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1407 - loss: 2.9890 - val_accuracy: 0.2727 - val_loss: 2.2393 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2191 - loss: 2.8075 - val_accuracy: 0.2614 - val_loss: 2.1942 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2213 - loss: 2.6430 - val_accuracy: 0.2727 - val_loss: 2.1513 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2652 - loss: 2.3548 - val_accuracy: 0.2727 - val_loss: 2.1150 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2371 - loss: 2.4461 - val_accuracy: 0.2841 - val_loss: 2.0758 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3143 - loss: 2.2401 - val_accuracy: 0.2841 - val_loss: 2.0395 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2943 - loss: 2.2142 - val_accuracy: 0.2841 - val_loss: 2.0050 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2777 - loss: 2.1669 - val_accuracy: 0.3068 - val_loss: 1.9730 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3354 - loss: 2.1571 - val_accuracy: 0.3182 - val_loss: 1.9432 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3734 - loss: 1.9462 - val_accuracy: 0.3295 - val_loss: 1.9134 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2872 - loss: 2.0915 - val_accuracy: 0.3068 - val_loss: 1.8828 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3381 - loss: 2.0320 - val_accuracy: 0.3409 - val_loss: 1.8485 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3604 - loss: 1.8445 - val_accuracy: 0.3523 - val_loss: 1.8221 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4329 - loss: 1.7451 - val_accuracy: 0.3523 - val_loss: 1.7842 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3293 - loss: 1.9662 - val_accuracy: 0.3636 - val_loss: 1.7551 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3667 - loss: 1.7177 - val_accuracy: 0.3750 - val_loss: 1.7291 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3536 - loss: 1.8877 - val_accuracy: 0.3750 - val_loss: 1.7055 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3972 - loss: 1.7931 - val_accuracy: 0.3977 - val_loss: 1.6862 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4458 - loss: 1.6059 - val_accuracy: 0.4091 - val_loss: 1.6608 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3679 - loss: 1.6682 - val_accuracy: 0.4205 - val_loss: 1.6269 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4113 - loss: 1.6725 - val_accuracy: 0.4205 - val_loss: 1.6022 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3907 - loss: 1.6469 - val_accuracy: 0.4205 - val_loss: 1.5793 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3678 - loss: 1.7824 - val_accuracy: 0.4205 - val_loss: 1.5599 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4060 - loss: 1.6146 - val_accuracy: 0.4205 - val_loss: 1.5450 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4099 - loss: 1.6428 - val_accuracy: 0.4318 - val_loss: 1.5230 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3920 - loss: 1.6070 - val_accuracy: 0.4318 - val_loss: 1.5035 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3777 - loss: 1.7366 - val_accuracy: 0.4432 - val_loss: 1.4817 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4603 - loss: 1.6057 - val_accuracy: 0.4432 - val_loss: 1.4656 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4204 - loss: 1.4447 - val_accuracy: 0.4318 - val_loss: 1.4540 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4332 - loss: 1.5178 - val_accuracy: 0.4318 - val_loss: 1.4503 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4602 - loss: 1.4129 - val_accuracy: 0.4886 - val_loss: 1.4385 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5447 - loss: 1.3834 - val_accuracy: 0.4886 - val_loss: 1.4239 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4365 - loss: 1.5296 - val_accuracy: 0.4886 - val_loss: 1.4172 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4727 - loss: 1.4948 - val_accuracy: 0.4773 - val_loss: 1.4118 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5116 - loss: 1.4468 - val_accuracy: 0.4659 - val_loss: 1.4019 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4673 - loss: 1.3652 - val_accuracy: 0.4659 - val_loss: 1.3994 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4644 - loss: 1.4315 - val_accuracy: 0.4432 - val_loss: 1.4038 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5408 - loss: 1.2632 - val_accuracy: 0.4659 - val_loss: 1.4001 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4918 - loss: 1.3406 - val_accuracy: 0.4545 - val_loss: 1.4040 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4360 - loss: 1.5000 - val_accuracy: 0.4545 - val_loss: 1.4081 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4651 - loss: 1.3445 - val_accuracy: 0.4773 - val_loss: 1.4088 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5523 - loss: 1.2036 - val_accuracy: 0.4659 - val_loss: 1.4019 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4328 - loss: 1.4097 - val_accuracy: 0.4773 - val_loss: 1.3945 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4265 - loss: 1.4090 - val_accuracy: 0.4773 - val_loss: 1.3874 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5462 - loss: 1.3214 - val_accuracy: 0.4773 - val_loss: 1.3883 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4872 - loss: 1.3862 - val_accuracy: 0.5000 - val_loss: 1.3867 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5085 - loss: 1.3445 - val_accuracy: 0.5000 - val_loss: 1.3810 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4855 - loss: 1.3457 - val_accuracy: 0.5000 - val_loss: 1.3857 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4537 - loss: 1.4006 - val_accuracy: 0.5000 - val_loss: 1.3760 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5030 - loss: 1.3307 - val_accuracy: 0.5000 - val_loss: 1.3733 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5682 - loss: 1.2218 - val_accuracy: 0.5000 - val_loss: 1.3732 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5439 - loss: 1.2342 - val_accuracy: 0.5114 - val_loss: 1.3645 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5199 - loss: 1.3374 - val_accuracy: 0.5000 - val_loss: 1.3573 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4659 - loss: 1.3964 - val_accuracy: 0.5114 - val_loss: 1.3528 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5088 - loss: 1.2946 - val_accuracy: 0.5000 - val_loss: 1.3609 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4987 - loss: 1.2896 - val_accuracy: 0.4886 - val_loss: 1.3710 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5058 - loss: 1.1606 - val_accuracy: 0.4886 - val_loss: 1.3761 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4796 - loss: 1.3071 - val_accuracy: 0.5000 - val_loss: 1.3789 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5511 - loss: 1.1609 - val_accuracy: 0.5000 - val_loss: 1.3781 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4832 - loss: 1.3031 - val_accuracy: 0.5000 - val_loss: 1.3798 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5198 - loss: 1.1640 - val_accuracy: 0.4886 - val_loss: 1.3766 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5158 - loss: 1.2580 - val_accuracy: 0.5114 - val_loss: 1.3663 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5527 - loss: 1.1596 - val_accuracy: 0.5114 - val_loss: 1.3587 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5016 - loss: 1.3078 - val_accuracy: 0.4886 - val_loss: 1.3544 - learning_rate: 0.0010\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Model Accuracy: 0.4954954954954955\n",
      "Model F1 Score: 0.4682872289255267\n",
      "Model Precision: 0.48647623715191285\n",
      "Model Recall: 0.4954954954954955\n",
      "Model ROC AUC: 0.8652792077230937\n",
      "Model MCC: 0.39161388664775576\n",
      "Model Cohen Kappa: 0.3866193013617527\n",
      "Model MAE: 2.0720720720720722\n",
      "Model NMAE: 0.20720720720720723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('Tvet_dataRegular.xlsx')\n",
    "\n",
    "# Encode categorical variables\n",
    "le_gender = LabelEncoder()\n",
    "df['Gender'] = le_gender.fit_transform(df['Gender'])\n",
    "\n",
    "le_trade = LabelEncoder()\n",
    "df['Trade'] = le_trade.fit_transform(df['Trade'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('Trade', axis=1)\n",
    "y = df['Trade']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert target to categorical\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Build the model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(y_train_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_categorical, epochs=100, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Decode the predicted and actual labels\n",
    "y_test_decoded = le_trade.inverse_transform(y_test)\n",
    "y_pred_decoded = le_trade.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate MAE and NMAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "nmae = mae / (y.max() - y.min())\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(to_categorical(y_test), y_pred_proba, average='weighted', multi_class='ovr')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Model Accuracy: {accuracy}')\n",
    "print(f'Model F1 Score: {f1}')\n",
    "print(f'Model Precision: {precision}')\n",
    "print(f'Model Recall: {recall}')\n",
    "print(f'Model ROC AUC: {roc_auc}')\n",
    "print(f'Model MCC: {mcc}')\n",
    "print(f'Model Cohen Kappa: {cohen_kappa}')\n",
    "print(f'Model MAE: {mae}')\n",
    "print(f'Model NMAE: {nmae}')\n",
    "\n",
    "# Save predictions to Excel\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test_decoded,\n",
    "    'Predicted': y_pred_decoded\n",
    "})\n",
    "predictions_df.to_excel('DeepLearning_Predictions.xlsx', index=False)\n",
    "\n",
    "# Save evaluation metrics to Excel\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC AUC', 'MCC', 'Cohen Kappa', 'MAE', 'NMAE'],\n",
    "    'Score': [accuracy, f1, precision, recall, roc_auc, mcc, cohen_kappa, mae, nmae]\n",
    "})\n",
    "metrics_df.to_excel('Evaluation_Metrics.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b680d030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 28s]\n",
      "val_accuracy: 0.4027777810891469\n",
      "\n",
      "Best val_accuracy So Far: 0.5069444477558136\n",
      "Total elapsed time: 00h 02m 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 36 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.8532 - loss: 1.3343 - val_accuracy: 0.4792 - val_loss: 2.4520 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8450 - loss: 1.2367 - val_accuracy: 0.4792 - val_loss: 2.4437 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8824 - loss: 1.1662 - val_accuracy: 0.5000 - val_loss: 2.4698 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8384 - loss: 1.2689 - val_accuracy: 0.5208 - val_loss: 2.4628 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9097 - loss: 1.1610 - val_accuracy: 0.4792 - val_loss: 2.4670 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8718 - loss: 1.2218 - val_accuracy: 0.4375 - val_loss: 2.4438 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9152 - loss: 1.1324 - val_accuracy: 0.4583 - val_loss: 2.4428 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8815 - loss: 1.1759 - val_accuracy: 0.4583 - val_loss: 2.4983 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8651 - loss: 1.2741 - val_accuracy: 0.4792 - val_loss: 2.5697 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9009 - loss: 1.1510 - val_accuracy: 0.5000 - val_loss: 2.6094 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8486 - loss: 1.2259 - val_accuracy: 0.4583 - val_loss: 2.5770 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9170 - loss: 1.0900 - val_accuracy: 0.4583 - val_loss: 2.5635 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8965 - loss: 1.1164 - val_accuracy: 0.5417 - val_loss: 2.5309 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8867 - loss: 1.1296 - val_accuracy: 0.5208 - val_loss: 2.5501 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8837 - loss: 1.1804 - val_accuracy: 0.5000 - val_loss: 2.6332 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9052 - loss: 1.1082 - val_accuracy: 0.5000 - val_loss: 2.7148 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9111 - loss: 1.0727 - val_accuracy: 0.4583 - val_loss: 2.7522 - learning_rate: 0.0010\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000293A2607E20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000293A2607E20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "Model Accuracy: 0.4666666666666667\n",
      "Model F1 Score: 0.4581622678396871\n",
      "Model Precision: 0.5257843137254902\n",
      "Model Recall: 0.4666666666666667\n",
      "Model ROC AUC: 0.8402018771798705\n",
      "Model MCC: 0.3691720066364861\n",
      "Model Cohen Kappa: 0.3554884189325277\n",
      "Model MAE: 1.5833333333333333\n",
      "Model NMAE: 0.2638888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#More Complex (Working Now)\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten, Concatenate, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Remove the existing tuner project directory if it exists\n",
    "project_dir = 'keras_tuner_dir/tvet_course_recommender'\n",
    "if os.path.exists(project_dir):\n",
    "    shutil.rmtree(project_dir)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('Tvet_data.xlsx')\n",
    "\n",
    "# Encode categorical variables\n",
    "le_gender = LabelEncoder()\n",
    "df['Gender'] = le_gender.fit_transform(df['Gender'])\n",
    "\n",
    "le_trade = LabelEncoder()\n",
    "df['Trade'] = le_trade.fit_transform(df['Trade'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('Trade', axis=1)\n",
    "y = df['Trade']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert target to categorical\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Embedding input dimensions\n",
    "gender_input_dim = len(le_gender.classes_)\n",
    "\n",
    "# Build the hyperparameter tuning model\n",
    "def build_model(hp):\n",
    "    # Input layers\n",
    "    profile_code_input = Input(shape=(1,), name='ProfileCode')\n",
    "    gender_input = Input(shape=(1,), name='Gender')\n",
    "    age_input = Input(shape=(1,), name='Age')\n",
    "    score_e_input = Input(shape=(1,), name='Score_E')\n",
    "    score_a_input = Input(shape=(1,), name='Score_A')\n",
    "    score_c_input = Input(shape=(1,), name='Score_C')\n",
    "    score_n_input = Input(shape=(1,), name='Score_N')\n",
    "    score_o_input = Input(shape=(1,), name='Score_O')\n",
    "\n",
    "    # Embedding layers\n",
    "    gender_embedding = Embedding(input_dim=gender_input_dim, output_dim=hp.Int('gender_emb_dim', min_value=4, max_value=16, step=4))(gender_input)\n",
    "    gender_embedding = Flatten()(gender_embedding)\n",
    "    \n",
    "    # Concatenate all inputs\n",
    "    concatenated = Concatenate()([profile_code_input, gender_embedding, age_input, score_e_input, score_a_input, score_c_input, score_n_input, score_o_input])\n",
    "    \n",
    "    # Fully connected layers with attention\n",
    "    x = Dense(units=hp.Int('units_input', min_value=128, max_value=512, step=64), activation='relu', kernel_regularizer=l2(0.001))(concatenated)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('dropout_input', 0.3, 0.7, step=0.1))(x)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 2, 5)):\n",
    "        x = Dense(units=hp.Int(f'units_{i}', min_value=64, max_value=512, step=64), activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}', 0.3, 0.7, step=0.1))(x)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = Dense(1, activation='tanh')(x)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    x = tf.keras.layers.multiply([x, attention])\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(y_train_categorical.shape[1], activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[profile_code_input, gender_input, age_input, score_e_input, score_a_input, score_c_input, score_n_input, score_o_input], outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Set up the tuner with Bayesian optimization\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model, \n",
    "    objective='val_accuracy', \n",
    "    max_trials=5, \n",
    "    executions_per_trial=3, \n",
    "    directory='keras_tuner_dir', \n",
    "    project_name='tvet_course_recommender',\n",
    "    overwrite=True  # Allow overwriting the existing project\n",
    ")\n",
    "\n",
    "# Prepare the data for the model\n",
    "train_data = {\n",
    "    'ProfileCode': X_train[:, 0],\n",
    "    'Gender': X_train[:, 1],\n",
    "    'Age': X_train[:, 2],\n",
    "    'Score_E': X_train[:, 3],\n",
    "    'Score_A': X_train[:, 4],\n",
    "    'Score_C': X_train[:, 5],\n",
    "    'Score_N': X_train[:, 6],\n",
    "    'Score_O': X_train[:, 7],\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'ProfileCode': X_test[:, 0],\n",
    "    'Gender': X_test[:, 1],\n",
    "    'Age': X_test[:, 2],\n",
    "    'Score_E': X_test[:, 3],\n",
    "    'Score_A': X_test[:, 4],\n",
    "    'Score_C': X_test[:, 5],\n",
    "    'Score_N': X_test[:, 6],\n",
    "    'Score_O': X_test[:, 7],\n",
    "}\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(train_data, y_train_categorical, epochs=100, validation_split=0.2, verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_model.h5')\n",
    "\n",
    "# Load the best model and recreate the optimizer\n",
    "best_model = tf.keras.models.load_model('best_model.h5')\n",
    "best_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the best model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = best_model.fit(train_data, y_train_categorical, epochs=100, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = best_model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Decode the predicted and actual labels\n",
    "y_test_decoded = le_trade.inverse_transform(y_test)\n",
    "y_pred_decoded = le_trade.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate MAE and NMAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "nmae = mae / (y.max() - y.min())\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(to_categorical(y_test), y_pred_proba, average='weighted', multi_class='ovr')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Model Accuracy: {accuracy}')\n",
    "print(f'Model F1 Score: {f1}')\n",
    "print(f'Model Precision: {precision}')\n",
    "print(f'Model Recall: {recall}')\n",
    "print(f'Model ROC AUC: {roc_auc}')\n",
    "print(f'Model MCC: {mcc}')\n",
    "print(f'Model Cohen Kappa: {cohen_kappa}')\n",
    "print(f'Model MAE: {mae}')\n",
    "print(f'Model NMAE: {nmae}')\n",
    "\n",
    "# Save predictions to Excel\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test_decoded,\n",
    "    'Predicted': y_pred_decoded\n",
    "})\n",
    "predictions_df.to_excel('DeepLearning_Predictions.xlsx', index=False)\n",
    "\n",
    "# Save evaluation metrics to Excel\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC AUC', 'MCC', 'Cohen Kappa', 'MAE', 'NMAE'],\n",
    "    'Score': [accuracy, f1, precision, recall, roc_auc, mcc, cohen_kappa, mae, nmae]\n",
    "})\n",
    "metrics_df.to_excel('Evaluation_Metrics.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925369b",
   "metadata": {},
   "source": [
    "June 05, 2024 Updated Code with Malik Sb. Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a95a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras-tuner in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (1.4.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (1.5.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (2.16.1)\n",
      "Requirement already satisfied: keras in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from keras-tuner) (3.3.3)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-tuner) (23.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-tuner) (2.31.0)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from keras->keras-tuner) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from keras->keras-tuner) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2023.7.22)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras->keras-tuner) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras->keras-tuner) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner scikit-learn pandas numpy tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1097814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 26s]\n",
      "val_accuracy: 0.7017543911933899\n",
      "\n",
      "Best val_accuracy So Far: 0.9122806986172994\n",
      "Total elapsed time: 00h 08m 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 36 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 180ms/step - accuracy: 0.9931 - loss: 0.5934 - val_accuracy: 0.8421 - val_loss: 0.8693 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9813 - loss: 0.5890 - val_accuracy: 0.8421 - val_loss: 0.8446 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9783 - loss: 0.6040 - val_accuracy: 0.9474 - val_loss: 0.7989 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.5593 - val_accuracy: 0.9474 - val_loss: 0.7630 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9635 - loss: 0.6026 - val_accuracy: 0.9474 - val_loss: 0.7292 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9644 - loss: 0.6323 - val_accuracy: 0.9474 - val_loss: 0.7162 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9813 - loss: 0.5883 - val_accuracy: 0.8947 - val_loss: 0.7269 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9891 - loss: 0.5596 - val_accuracy: 0.8947 - val_loss: 0.7512 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9813 - loss: 0.5777 - val_accuracy: 0.8421 - val_loss: 0.7713 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9714 - loss: 0.5800 - val_accuracy: 0.8421 - val_loss: 0.7935 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9596 - loss: 0.6019 - val_accuracy: 0.7895 - val_loss: 0.8071 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9891 - loss: 0.5611 - val_accuracy: 0.7895 - val_loss: 0.7954 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9813 - loss: 0.5637 - val_accuracy: 0.7895 - val_loss: 0.7914 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.5337 - val_accuracy: 0.8421 - val_loss: 0.7808 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9705 - loss: 0.5945 - val_accuracy: 0.8947 - val_loss: 0.7537 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9744 - loss: 0.5711 - val_accuracy: 0.8947 - val_loss: 0.7419 - learning_rate: 0.0010\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (23, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7391304347826086\n",
      "Model F1 Score: 0.7328130806391675\n",
      "Model Precision: 0.7575250836120402\n",
      "Model Recall: 0.7391304347826086\n",
      "Model ROC AUC: 0.9248665141113652\n",
      "Model MCC: 0.5513698630136986\n",
      "Model Cohen Kappa: 0.5384615384615384\n",
      "Model MAE: 0.30434782608695654\n",
      "Model NMAE: 0.15217391304347827\n",
      "Model RMSE: 0.6255432421712243\n",
      "Model MRR: 0.3768115942028985\n",
      "Model NDCG: 0.9037208052795107\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'top_n_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 221\u001b[0m\n\u001b[0;32m    216\u001b[0m predictions_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeepLearning_Predictions.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Save evaluation metrics to Excel\u001b[39;00m\n\u001b[0;32m    219\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROC AUC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMCC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCohen Kappa\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNMAE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop N Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMRR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision at K\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m: [accuracy, f1, precision, recall, roc_auc, mcc, cohen_kappa, mae, nmae, rmse, top_n_acc, mrr_score, ndcg, prec_at_k]\n\u001b[0;32m    222\u001b[0m })\n\u001b[0;32m    223\u001b[0m metrics_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluation_Metrics.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'top_n_acc' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score, mean_absolute_error\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten, Concatenate, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Function to calculate Top N Accuracy\n",
    "def top_n_accuracy(y_true, y_pred_proba, n=3):\n",
    "    top_n_preds = np.argsort(y_pred_proba, axis=1)[:, -n:]\n",
    "    top_n_accuracy = np.mean([1 if y_true[i] in top_n_preds[i] else 0 for i in range(len(y_true))])\n",
    "    return top_n_accuracy\n",
    "\n",
    "# Function to calculate Mean Reciprocal Rank (MRR)\n",
    "def mrr(y_true, y_pred_proba):\n",
    "    order = np.argsort(y_pred_proba, axis=1)\n",
    "    ranks = np.where(order == np.expand_dims(y_true, axis=1))[1]\n",
    "    return np.mean(1.0 / (ranks + 1))\n",
    "\n",
    "# Function to calculate Precision at K\n",
    "def precision_at_k(y_true, y_pred_proba, k=5):\n",
    "    top_k_preds = np.argsort(y_pred_proba, axis=1)[:, -k:]\n",
    "    precision_at_k = np.mean([1 if y_true[i] in top_k_preds[i] else 0 for i in range(len(y_true))])\n",
    "    return precision_at_k\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('Tvet_dataCBT.xlsx')\n",
    "\n",
    "# Encode categorical variables\n",
    "le_gender = LabelEncoder()\n",
    "df['Gender'] = le_gender.fit_transform(df['Gender'])\n",
    "\n",
    "le_trade = LabelEncoder()\n",
    "df['Trade'] = le_trade.fit_transform(df['Trade'])\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('Trade', axis=1)\n",
    "y = df['Trade']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert target to categorical\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "\n",
    "# Embedding input dimensions\n",
    "gender_input_dim = len(le_gender.classes_)\n",
    "\n",
    "# Build the hyperparameter tuning model\n",
    "def build_model(hp):\n",
    "    # Input layers\n",
    "    profile_code_input = Input(shape=(1,), name='ProfileCode')\n",
    "    gender_input = Input(shape=(1,), name='Gender')\n",
    "    age_input = Input(shape=(1,), name='Age')\n",
    "    score_e_input = Input(shape=(1,), name='Score_E')\n",
    "    score_a_input = Input(shape=(1,), name='Score_A')\n",
    "    score_c_input = Input(shape=(1,), name='Score_C')\n",
    "    score_n_input = Input(shape=(1,), name='Score_N')\n",
    "    score_o_input = Input(shape=(1,), name='Score_O')\n",
    "\n",
    "    # Embedding layers\n",
    "    gender_embedding = Embedding(input_dim=gender_input_dim, output_dim=hp.Int('gender_emb_dim', min_value=4, max_value=16, step=4))(gender_input)\n",
    "    gender_embedding = Flatten()(gender_embedding)\n",
    "    \n",
    "    # Concatenate all inputs\n",
    "    concatenated = Concatenate()([profile_code_input, gender_embedding, age_input, score_e_input, score_a_input, score_c_input, score_n_input, score_o_input])\n",
    "    \n",
    "    # Fully connected layers with attention\n",
    "    x = Dense(units=hp.Int('units_input', min_value=128, max_value=512, step=64), activation='relu', kernel_regularizer=l2(0.001))(concatenated)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('dropout_input', 0.3, 0.7, step=0.1))(x)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 2, 5)):\n",
    "        x = Dense(units=hp.Int(f'units_{i}', min_value=64, max_value=512, step=64), activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}', 0.3, 0.7, step=0.1))(x)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = Dense(1, activation='tanh')(x)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    x = tf.keras.layers.multiply([x, attention])\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(y_train_categorical.shape[1], activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[profile_code_input, gender_input, age_input, score_e_input, score_a_input, score_c_input, score_n_input, score_o_input], outputs=output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Set up the tuner with Bayesian optimization\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model, \n",
    "    objective='val_accuracy', \n",
    "    max_trials=20, \n",
    "    executions_per_trial=3, \n",
    "    directory='keras_tuner_dir', \n",
    "    project_name='tvet_course_recommender',\n",
    "    overwrite=True  # Allow overwriting the existing project\n",
    ")\n",
    "\n",
    "# Prepare the data for the model\n",
    "train_data = {\n",
    "    'ProfileCode': X_train[:, 0],\n",
    "    'Gender': X_train[:, 1],\n",
    "    'Age': X_train[:, 2],\n",
    "    'Score_E': X_train[:, 3],\n",
    "    'Score_A': X_train[:, 4],\n",
    "    'Score_C': X_train[:, 5],\n",
    "    'Score_N': X_train[:, 6],\n",
    "    'Score_O': X_train[:, 7],\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'ProfileCode': X_test[:, 0],\n",
    "    'Gender': X_test[:, 1],\n",
    "    'Age': X_test[:, 2],\n",
    "    'Score_E': X_test[:, 3],\n",
    "    'Score_A': X_test[:, 4],\n",
    "    'Score_C': X_test[:, 5],\n",
    "    'Score_N': X_test[:, 6],\n",
    "    'Score_O': X_test[:, 7],\n",
    "}\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(train_data, y_train_categorical, epochs=100, validation_split=0.2, verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_model.h5')\n",
    "\n",
    "# Load the best model and recreate the optimizer\n",
    "best_model = tf.keras.models.load_model('best_model.h5')\n",
    "best_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the best model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "history = best_model.fit(train_data, y_train_categorical, epochs=100, batch_size=32, validation_split=0.2, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = best_model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Decode the predicted and actual labels\n",
    "y_test_decoded = le_trade.inverse_transform(y_test)\n",
    "y_pred_decoded = le_trade.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate MAE and NMAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "nmae = mae / (y.max() - y.min())\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Calculate Top N Accuracy\n",
    "#top_n_acc = top_n_accuracy(y_test, y_pred_proba, n=3)\n",
    "\n",
    "# Calculate MRR\n",
    "mrr_score = mrr(y_test, y_pred_proba)\n",
    "\n",
    "# Calculate NDCG\n",
    "ndcg = ndcg_score(to_categorical(y_test), y_pred_proba, k=5)\n",
    "\n",
    "# Calculate Precision at K\n",
    "#prec_at_k = precision_at_k(y_test, y_pred_proba, k=5)\n",
    "\n",
    "# Evaluate the model using traditional metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(to_categorical(y_test), y_pred_proba, average='weighted', multi_class='ovr')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "cohen_kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Model Accuracy: {accuracy}')\n",
    "print(f'Model F1 Score: {f1}')\n",
    "print(f'Model Precision: {precision}')\n",
    "print(f'Model Recall: {recall}')\n",
    "print(f'Model ROC AUC: {roc_auc}')\n",
    "print(f'Model MCC: {mcc}')\n",
    "print(f'Model Cohen Kappa: {cohen_kappa}')\n",
    "print(f'Model MAE: {mae}')\n",
    "print(f'Model NMAE: {nmae}')\n",
    "print(f'Model RMSE: {rmse}')\n",
    "#print(f'Model Top N Accuracy: {top_n_acc}')\n",
    "print(f'Model MRR: {mrr_score}')\n",
    "print(f'Model NDCG: {ndcg}')\n",
    "#print(f'Model Precision at K: {prec_at_k}')\n",
    "\n",
    "# Save predictions to Excel\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test_decoded,\n",
    "    'Predicted': y_pred_decoded\n",
    "})\n",
    "predictions_df.to_excel('DeepLearning_Predictions.xlsx', index=False)\n",
    "\n",
    "# Save evaluation metrics to Excel\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC AUC', 'MCC', 'Cohen Kappa', 'MAE', 'NMAE', 'RMSE', 'Top N Accuracy', 'MRR', 'NDCG', 'Precision at K'],\n",
    "    'Score': [accuracy, f1, precision, recall, roc_auc, mcc, cohen_kappa, mae, nmae, rmse, top_n_acc, mrr_score, ndcg, prec_at_k]\n",
    "})\n",
    "metrics_df.to_excel('Evaluation_Metrics.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a51c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
